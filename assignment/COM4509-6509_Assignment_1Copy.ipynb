{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Brief \n",
    "\n",
    "## Deadline: Tuesday, October 29, 2019 at 14:00 hrs\n",
    "\n",
    "## Number of marks available: 20\n",
    "\n",
    "## Scope: Sessions 1 to 5\n",
    "\n",
    "### How and what to submit\n",
    "\n",
    "A. Submit a Jupyter Notebook named COM4509-6509_Assignment_1_UCard_XXXXXXXXX.ipynb where XXXXXXXXX refers to your UCard number.\n",
    "\n",
    "B. Upload the notebook file to MOLE before the deadline above.\n",
    "\n",
    "C. **NO DATA UPLOAD**: Please do not upload the data files used. We have a copy already. \n",
    "\n",
    "\n",
    "### Assessment Criteria \n",
    "\n",
    "* Being able to express an objective function and its gradients in matrix form.\n",
    "\n",
    "* Being able to use numpy and pandas to preprocess a dataset.\n",
    "\n",
    "* Being able to use numpy to build a machine learning pipeline for supervised learning. \n",
    "\n",
    "\n",
    "### Late submissions\n",
    "\n",
    "We follow Department's guidelines about late submissions, i.e., a deduction of 5% of the mark each working day the work is late after the deadline. NO late submission will be marked one week after the deadline because we will release a solution by then. Please read [this link](https://sites.google.com/sheffield.ac.uk/compgtstudenthandbook/menu/assessment/late-submission?pli=1&authuser=1). \n",
    "\n",
    "### Use of unfair means \n",
    "\n",
    "**\"Any form of unfair means is treated as a serious academic offence and action may be taken under the Discipline Regulations.\"** (from the MSc Handbook). Please carefully read [this link](https://sites.google.com/sheffield.ac.uk/compgtstudenthandbook/menu/referencing-unfair-means?pli=1&authuser=1) on what constitutes Unfair Means if not sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation for Linear Regression\n",
    "\n",
    "Regularisation is a technique commonly used in Machine Learning to prevent overfitting. It consists on adding terms to the objective function such that the optimisation procedure avoids solutions that just learn the training data. Popular techniques for regularisation in Supervised Learning include Lasso Regression, Ridge Regression and the Elastic Net. \n",
    "\n",
    "In this Assignment, you will be looking at Ridge Regression and devising equations to optimise the objective function in Ridge Regression using two methods: a closed-form derivation and the update rules for stochastic gradient descent. You will then use those update rules for making predictions on a Air Quaility dataset.\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "Let us start with a data set for training $\\mathcal{D} = \\{\\mathbf{y}, \\mathbf{X}\\}$, where the vector $\\mathbf{y}=[y_1, \\cdots, y_n]^{\\top}$ and $\\mathbf{X}$ is the design matrix from Lab 3, this is, \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X} = \n",
    "                \\begin{bmatrix}\n",
    "                        1 & x_{1,1} & \\cdots & x_{1, D}\\\\\n",
    "                        1 & x_{2,1} & \\cdots & x_{2, D}\\\\\n",
    "                   \\vdots &  \\vdots\\\\\n",
    "                        1 & x_{n,1} & \\cdots & x_{n, D}\n",
    "                \\end{bmatrix}\n",
    "               = \n",
    "               \\begin{bmatrix}\n",
    "                      \\mathbf{x}_1^{\\top}\\\\\n",
    "                       \\mathbf{x}_2^{\\top}\\\\\n",
    "                          \\vdots\\\\\n",
    "                        \\mathbf{x}_n^{\\top}\n",
    "                \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Our predictive model is going to be a linear model\n",
    "\n",
    "$$ f(\\mathbf{x}_i) = \\mathbf{w}^{\\top}\\mathbf{x}_i,$$\n",
    "\n",
    "where $\\mathbf{w} = [w_0\\; w_1\\; \\cdots \\; w_D]^{\\top}$.\n",
    "\n",
    "The **objetive function** we are going to use has the following form\n",
    "\n",
    "$$ J(\\mathbf{w}, \\alpha) = \\frac{1}{n}\\sum_{i=1}^n (y_i - f(\\mathbf{x}_i))^2 + \\frac{\\alpha}{2}\\sum_{j=0}^D w_j^2,$$\n",
    "\n",
    "where $\\alpha>0$ is known as the *regularisation* parameter.\n",
    "\n",
    "The first term on the right-hand side (rhs) of the expression for $J(\\mathbf{w}, \\alpha)$ is very similar to the least-squares objective function we have seen before, for example in Lab 3. The only difference is on the term $\\frac{1}{n}$ that we use to normalise the objective with respect to the number of observations in the dataset. \n",
    "\n",
    "The first term on the rhs is what we call the \"fitting\" term whereas the second term in the expression is the regularisation term. Given $\\alpha$, the two terms in the expression have different purposes. The first term is looking for a value of $\\mathbf{w}$ that leads the squared-errors to zero. While doing this, $\\mathbf{w}$ can take any value and lead to a solution that it is only good for the training data but perhaps not for the test data. The second term is regularising the behavior of the first term by driving the $\\mathbf{w}$ towards zero. By doing this, it restricts the possible set of values that $\\mathbf{w}$ might take according to the first term. The value that we use for $\\alpha$ will allow a compromise between a value of $\\mathbf{w}$ that exactly fits the data (first term) or a value of $\\mathbf{w}$ that does not grow too much (second term).\n",
    "\n",
    "This type of regularisation has different names: ridge regression, Tikhonov regularisation or $\\ell_2$ norm regularisation. \n",
    "\n",
    "### Question 1: $J(\\mathbf{w}, \\alpha)$ in matrix form (2 marks)\n",
    "\n",
    "Write the expression for $J(\\mathbf{w}, \\alpha)$ in matrix form. Include ALL the steps necessary to reach the expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 Answer\n",
    "\n",
    "$$ J(\\mathbf{w}, \\alpha) = \\frac{1}{n}\\sum_{i=1}^n (y_i - f(\\mathbf{x}_i))^2 + \\frac{\\alpha}{2}\\sum_{j=0}^D w_j^2,$$\n",
    "\n",
    "First SumÔºö\n",
    "all we need to do is convert the sum operator to an inner product. We can get a vector from that sum operator by placing both  ùë¶ùëñ  and  ùëì(ùê±ùëñ)  into vector\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix}y_1\\\\y_2\\\\ \\vdots \\\\ y_n\\end{bmatrix}\n",
    "$$\n",
    "and defining\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{X}) = \\begin{bmatrix}f(\\mathbf{x}_1)\\\\f(\\mathbf{x}_2)\\\\ \\vdots \\\\ f(\\mathbf{x}_n)\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "for the moment, ignore the dependence of $\\mathbf{f}$ $\\mathbf{X}$ and simply summarise it by a vector of numbers\n",
    "$$\n",
    "\\mathbf{f} = \\begin{bmatrix}f_1\\\\f_2\\\\ \\vdots \\\\ f_n\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "so the first sum of this equation can be written as :\n",
    "$$\n",
    "\\frac{(\\mathbf{y} - \\mathbf{f})^\\top(\\mathbf{y} - \\mathbf{f})}{n}\n",
    "$$\n",
    "\n",
    "Second Sum:\n",
    "convert the sum operator to an inner product. We can get a vector from that sum operator by placing Wj  into vector\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\begin{bmatrix}w_0\\\\w_1\\\\ \\vdots \\\\ w_D\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "so we have \n",
    "\n",
    "$$\n",
    "            \\frac{\\alpha}{2}\\sum_{j=0}^D w_j^2 = \\frac{\\alpha}{2}(\\mathbf{w}^T\\mathbf{w})\n",
    "$$\n",
    "\n",
    "Finally, the whole objective function in linear algebraic form:\n",
    "$$\n",
    "J(\\mathbf{w}, \\alpha) = \\frac{(\\mathbf{y} - \\mathbf{f})^\\top(\\mathbf{y} - \\mathbf{f})}{n}+\\frac{\\alpha(\\mathbf{w}^T\\mathbf{w})}{2}\n",
    "$$ \n",
    "\n",
    "since $$ f(\\mathbf{x}_i) = \\mathbf{w}^{\\top}\\mathbf{x}_i,$$\n",
    "\n",
    "the function can be written as:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, \\alpha) = \\frac{(\\mathbf{y} - \\mathbf{w}^{\\top}\\mathbf{x}_i)^\\top(\\mathbf{y} - \\mathbf{w}^{\\top}\\mathbf{x}_i)}{n}+\\frac{\\alpha(\\mathbf{w}^T\\mathbf{w})}{2}\n",
    "$$ \n",
    "\n",
    "\n",
    "In Matrix Expression:\n",
    "$$\n",
    "\\frac{(\\begin{bmatrix}y_1\\\\y_2\\\\ \\vdots \\\\ y_n\\end{bmatrix}-\\begin{bmatrix}f_1\\\\f_2\\\\ \\vdots \\\\ f_n\\end{bmatrix})^T\\begin{bmatrix}y_1\\\\y_2\\\\ \\vdots \\\\ y_n\\end{bmatrix}-\\begin{bmatrix}f_1\\\\f_2\\\\ \\vdots \\\\ f_n\\end{bmatrix}}{n}+\\frac{\\alpha(\\begin{bmatrix}w_0\\\\w_1\\\\ \\vdots \\\\ w_D\\end{bmatrix}^T\\begin{bmatrix}w_0\\\\w_1\\\\ \\vdots \\\\ w_D\\end{bmatrix})}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising the objective function with respect to $\\mathbf{w}$\n",
    "\n",
    "There are two ways we can optimise the objective function with respect to $\\mathbf{w}$. The first one leads to a closed form expression for $\\mathbf{w}$ and the second one using an iterative optimisation procedure that updates the value of $\\mathbf{w}$ at each iteration by using the gradient of the objective function with respect to $\\mathbf{w}$,\n",
    "$$\n",
    "\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d J(\\mathbf{w}, \\alpha)}{d\\mathbf{w}},\n",
    "$$\n",
    "where $\\eta$ is the *learning rate* parameter and $\\frac{d J(\\mathbf{w}, \\alpha)}{d\\mathbf{w}}$ is the gradient of the objective function.\n",
    "\n",
    "### Question 2: Derivative of $J(\\mathbf{w}, \\alpha)$ wrt $\\mathbf{w}$ (2 marks)\n",
    "\n",
    "Find the closed-form expression for $\\mathbf{w}$ by taking the derivative of $J(\\mathbf{w}, \\alpha)$ with respect to \n",
    "$\\mathbf{w}$, equating to zero and solving for $\\mathbf{w}$. Write the expression in matrix form. \n",
    "\n",
    "Also, write down the specific update rule for $\\mathbf{w}_{\\text{new}}$ by using the equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 Answer\n",
    "\n",
    "Write your answer to the question in this box.\n",
    "\n",
    "\n",
    "From question 1Ôºå we have\n",
    "$$\n",
    "J(\\mathbf{w}, \\alpha) = \\frac{(\\mathbf{y} - \\mathbf{w}^{\\top}\\mathbf{x}_i)^\\top(\\mathbf{y} - \\mathbf{w}^{\\top}\\mathbf{x}_i)}{n}+\\frac{\\alpha(\\mathbf{w}^T\\mathbf{w})}{2}\n",
    "$$ \n",
    "$$\n",
    "= \\frac{\\mathbf{y}^{\\top}\\mathbf{y}-\\mathbf{y}^{\\top}\\mathbf{w}^{\\top}\\mathbf{x}-(\\mathbf{w}^{\\top}\\mathbf{x})^{\\top}y+(\\mathbf{w}^{\\top}\\mathbf{x})^{\\top}\\mathbf{w}^{\\top}\\mathbf{x}}{n}+\\frac{\\alpha(\\mathbf{w}^T\\mathbf{w})}{2}\n",
    "$$\n",
    "\n",
    "Now, take the derivative:\n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{w}}J(\\mathbf{w,Œ±}) = \\frac{-\\mathbf{y}^{\\top}x -\\mathbf{x}^{\\top}\\mathbf{y}+\\mathbf{x}^{\\top}\\mathbf{w}^{\\top}\\mathbf{x}+\\mathbf{w}\\mathbf{x}^{\\top}\\mathbf{x}}{n}+{Œ±w}\n",
    "$$\n",
    "$$\n",
    "=\\frac{-\\mathbf{y}^{\\top}\\mathbf{x} -\\mathbf{x}y^{\\top}+\\mathbf{x}^{\\top}\\mathbf{w}^{\\top}\\mathbf{x}+\\mathbf{w}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}}{n}+{Œ±w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{-2\\mathbf{y}^{\\top}\\mathbf{x}+2\\mathbf{w}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}}{n}+{Œ±w}\n",
    "$$\n",
    "Final result:\n",
    "$$\n",
    "\\frac{\\text{d}}{\\text{d}\\mathbf{w}}J(\\mathbf{w,Œ±})=\\frac{2}{n}\n",
    "[\\,-\\mathbf{y}^{\\top}\\mathbf{x}+\\mathbf{w}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}]\\,+{Œ±w}\n",
    "$$\n",
    "\n",
    "equating to zero and solving for  ùê∞:\n",
    "$$\n",
    "0=\\frac{2}{n}\n",
    "[\\,-\\mathbf{y}^{\\top}\\mathbf{x}+\\mathbf{w}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}]\\,+{Œ±w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{2}{n}\\mathbf{w}\\mathbf{x}\\mathbf{x}^{\\top}+\\mathbf{Œ±}\\mathbf{w}= \\frac{2}{n}*\\mathbf{y}^{\\top}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{2}{n}\\mathbf{w}(\\mathbf{x}\\mathbf{x}^{\\top}+EŒ±) = \\frac{2}{n}\\mathbf{y}^{\\top}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{w}= \\frac{\\frac{2}{n}\\mathbf{y}^{\\top}\\mathbf{x}}{\\frac{2}{n}(\\mathbf{x}\\mathbf{x}^{\\top}+EŒ±)}=\\frac{\\mathbf{y}^{\\top}\\mathbf{x}}{(\\mathbf{x}\\mathbf{x}^{\\top}+\\frac{n}{2}EŒ±)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{w}=  (\\mathbf{x}\\mathbf{x}^{\\top}+\\frac{n}{2}EŒ±)^{-1} * \\mathbf{y}^{\\top}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "\n",
    "update rule for $\\mathbf{w}_{\\text{new}}$ :\n",
    "$$\n",
    "\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\eta (\\frac{2}{n}\n",
    "[\\,-\\mathbf{y}^{\\top}\\mathbf{x}+\\mathbf{w}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}]\\,+{Œ±w}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ridge regression to predict air quality\n",
    "\n",
    "Our dataset comes from a popular machine learning repository that hosts open source datasets for educational and research purposes, the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). We are going to use ridge regression for predicting air quality. The description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Air+Quality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "#pods.util.download_url('https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip')\n",
    "import zipfile\n",
    "zip = zipfile.ZipFile('./AirQualityUCI.zip', 'r')\n",
    "for name in zip.namelist():\n",
    "    zip.extract(name, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .csv version of the file has some typing issues, so we use the excel version\n",
    "import pandas as pd \n",
    "air_quality = pd.read_excel('./AirQualityUCI.xlsx', usecols=range(2,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the rows in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CO(GT)  PT08.S1(CO)  NMHC(GT)   C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  \\\n",
      "0        2.6  1360.000000       150  11.881723    1045.500000    166.0   \n",
      "1        2.0  1292.250000       112   9.397165     954.750000    103.0   \n",
      "2        2.2  1402.000000        88   8.997817     939.250000    131.0   \n",
      "3        2.2  1375.500000        80   9.228796     948.250000    172.0   \n",
      "4        1.6  1272.250000        51   6.518224     835.500000    131.0   \n",
      "5        1.2  1197.000000        38   4.741012     750.250000     89.0   \n",
      "6        1.2  1185.000000        31   3.624399     689.500000     62.0   \n",
      "7        1.0  1136.250000        31   3.326677     672.000000     62.0   \n",
      "8        0.9  1094.000000        24   2.339416     608.500000     45.0   \n",
      "9        0.6  1009.750000        19   1.696658     560.750000   -200.0   \n",
      "10    -200.0  1011.000000        14   1.293620     526.750000     21.0   \n",
      "11       0.7  1066.000000         8   1.133431     512.000000     16.0   \n",
      "12       0.7  1051.750000        16   1.603768     553.250000     34.0   \n",
      "13       1.1  1144.000000        29   3.243618     667.000000     98.0   \n",
      "14       2.0  1333.250000        64   8.013773     899.750000    174.0   \n",
      "15       2.2  1351.000000        87   9.540643     960.250000    129.0   \n",
      "16       1.7  1233.250000        77   6.335782     827.250000    112.0   \n",
      "17       1.5  1178.750000        43   4.971584     762.000000     95.0   \n",
      "18       1.6  1236.000000        61   5.216919     774.250000    104.0   \n",
      "19       1.9  1285.500000        63   7.269933     868.500000    146.0   \n",
      "20       2.9  1371.000000       164  11.539007    1033.500000    207.0   \n",
      "21       2.2  1310.000000        79   8.826223     932.500000    184.0   \n",
      "22       2.2  1291.750000        95   8.301413     911.500000    193.0   \n",
      "23       2.9  1383.000000       150  11.151581    1019.750000    243.0   \n",
      "24       4.8  1580.750000       307  20.799217    1318.500000    281.0   \n",
      "25       6.9  1775.500000       461  27.359807    1487.750000    383.0   \n",
      "26       6.1  1640.000000       401  24.017757    1404.000000    351.0   \n",
      "27       3.9  1312.750000       197  12.779368    1076.250000    240.0   \n",
      "28       1.5   964.500000        61   4.707072     748.500000     94.0   \n",
      "29       1.0   912.750000        26   2.645722     629.250000     47.0   \n",
      "...      ...          ...       ...        ...            ...      ...   \n",
      "9327     1.2  1022.250000      -200   4.082095     715.250000    191.9   \n",
      "9328     1.4   970.250000      -200   3.402194     676.500000    166.4   \n",
      "9329     1.3   999.750000      -200   5.313402     779.000000    171.3   \n",
      "9330     1.4   996.000000      -200   5.349138     780.750000    177.0   \n",
      "9331     1.0   928.000000      -200   3.817080     700.500000    118.7   \n",
      "9332     1.0   932.750000      -200   4.201281     721.750000    120.6   \n",
      "9333     1.1   956.000000      -200   5.395234     783.000000    141.9   \n",
      "9334     1.3   967.500000      -200   6.313819     826.250000    197.3   \n",
      "9335     1.4   952.500000      -200   6.101377     816.500000    241.8   \n",
      "9336     1.2  1014.666667      -200   4.594689     742.666667    190.1   \n",
      "9337     2.7  1248.000000      -200  11.109658    1018.250000    367.4   \n",
      "9338     2.5  1180.250000      -200   7.868564     893.750000    355.0   \n",
      "9339     1.5  1101.750000      -200   5.999005     811.750000    235.2   \n",
      "9340     1.6  1115.500000      -200   5.817660     803.250000    232.7   \n",
      "9341     1.2  1099.750000      -200   5.106145     768.750000    170.1   \n",
      "9342     0.9  1012.000000      -200   3.504006     682.500000    117.2   \n",
      "9343     0.6   944.250000      -200   1.931932     579.000000     69.6   \n",
      "9344     0.5   911.750000      -200   1.489323     543.750000     69.0   \n",
      "9345     0.4   887.000000      -200   1.086365     507.500000     61.7   \n",
      "9346  -200.0   864.250000      -200   0.801504     478.250000     52.3   \n",
      "9347     0.5   888.250000      -200   1.307608     528.000000     76.5   \n",
      "9348     1.1  1030.500000      -200   4.359341     730.250000    182.2   \n",
      "9349     4.0  1383.500000      -200  17.364240    1220.750000    593.7   \n",
      "9350     5.0  1446.000000      -200  22.393233    1361.500000    586.2   \n",
      "9351     3.9  1296.500000      -200  13.552393    1102.000000    522.7   \n",
      "9352     3.1  1314.250000      -200  13.529605    1101.250000    471.7   \n",
      "9353     2.4  1162.500000      -200  11.355157    1027.000000    353.3   \n",
      "9354     2.4  1142.000000      -200  12.374538    1062.500000    293.0   \n",
      "9355     2.1  1002.500000      -200   9.547187     960.500000    234.5   \n",
      "9356     2.2  1070.750000      -200  11.932060    1047.250000    265.2   \n",
      "\n",
      "      PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)          T         RH  \\\n",
      "0      1056.250000    113.0   1692.000000  1267.500000  13.600000  48.875001   \n",
      "1      1173.750000     92.0   1558.750000   972.250000  13.300000  47.700000   \n",
      "2      1140.000000    114.0   1554.500000  1074.000000  11.900000  53.975000   \n",
      "3      1092.000000    122.0   1583.750000  1203.250000  11.000000  60.000000   \n",
      "4      1205.000000    116.0   1490.000000  1110.000000  11.150000  59.575001   \n",
      "5      1336.500000     96.0   1393.000000   949.250000  11.175000  59.175000   \n",
      "6      1461.750000     77.0   1332.750000   732.500000  11.325000  56.775000   \n",
      "7      1453.250000     76.0   1332.750000   729.500000  10.675000  60.000000   \n",
      "8      1579.000000     60.0   1276.000000   619.500000  10.650000  59.674999   \n",
      "9      1705.000000   -200.0   1234.750000   501.250000  10.250000  60.200001   \n",
      "10     1817.500000     34.0   1196.750000   445.250000  10.075000  60.474999   \n",
      "11     1918.000000     28.0   1182.000000   421.750000  11.000000  56.175000   \n",
      "12     1738.250000     48.0   1221.250000   471.500000  10.450000  58.125000   \n",
      "13     1489.750000     82.0   1339.000000   729.750000  10.200000  59.599999   \n",
      "14     1136.000000    112.0   1517.000000  1101.500000  10.750000  57.425000   \n",
      "15     1079.000000    101.0   1582.750000  1027.750000  10.500000  60.599998   \n",
      "16     1218.000000     98.0   1445.750000   859.750000  10.800000  58.350000   \n",
      "17     1327.500000     92.0   1361.750000   670.500000  10.500000  57.925000   \n",
      "18     1301.250000     95.0   1401.250000   664.000000   9.525000  66.774999   \n",
      "19     1162.250000    112.0   1536.750000   799.000000   8.300000  76.425001   \n",
      "20      983.250000    128.0   1730.250000  1036.500000   8.000000  81.150000   \n",
      "21     1081.750000    126.0   1646.500000   946.250000   8.325000  79.799999   \n",
      "22     1102.500000    131.0   1590.750000   956.750000   9.700000  71.150002   \n",
      "23     1008.000000    135.0   1718.750000  1104.000000   9.775000  67.624998   \n",
      "24      798.500000    151.0   2083.000000  1408.500000  10.350000  64.174999   \n",
      "25      702.250000    172.0   2332.500000  1704.000000   9.650000  69.300001   \n",
      "26      742.750000    165.0   2191.250000  1653.750000   9.650000  67.750000   \n",
      "27      957.250000    136.0   1706.500000  1284.750000   9.125000  63.974999   \n",
      "28     1325.250000     85.0   1332.500000   821.000000   8.175000  63.400000   \n",
      "29     1564.500000     53.0   1252.250000   551.750000   8.250000  60.824999   \n",
      "...            ...      ...           ...          ...        ...        ...   \n",
      "9327    806.000000    118.0   1004.250000   830.750000  17.825000  30.675000   \n",
      "9328    887.500000    113.2    930.500000   612.500000  21.125000  23.725000   \n",
      "9329    804.500000    115.0   1000.750000   639.500000  24.000000  19.275000   \n",
      "9330    806.000000    123.9    985.250000   630.750000  26.450000  16.450000   \n",
      "9331    925.750000     86.4    902.000000   455.500000  28.675000  13.700000   \n",
      "9332    898.750000     87.0    889.500000   449.750000  28.475000  13.075000   \n",
      "9333    856.750000    100.4    895.750000   515.750000  30.000000  11.075000   \n",
      "9334    866.500000    132.0    898.250000   548.000000  29.425000  10.375000   \n",
      "9335    871.500000    156.3    891.000000   603.250000  28.875000   9.875000   \n",
      "9336    850.666667    137.9    980.666667   597.333333  22.800001  21.699999   \n",
      "9337    598.750000    181.3   1289.250000  1167.000000  19.925000  33.050000   \n",
      "9338    636.250000    186.8   1200.250000  1371.750000  17.500000  40.724999   \n",
      "9339    692.750000    157.8   1178.000000  1042.250000  16.450000  46.550000   \n",
      "9340    696.250000    153.2   1173.000000  1055.000000  15.525000  48.975000   \n",
      "9341    721.750000    127.6   1146.750000  1049.000000  14.275000  52.500000   \n",
      "9342    800.750000     93.0   1072.500000   816.250000  14.175000  51.450000   \n",
      "9343    924.750000     58.3   1001.750000   598.250000  13.850000  51.150001   \n",
      "9344    958.500000     54.6   1002.000000   573.000000  12.100000  56.300000   \n",
      "9345   1046.500000     51.3    973.500000   548.750000  11.325000  58.899999   \n",
      "9346   1116.000000     42.5    958.250000   488.750000  11.825000  55.975000   \n",
      "9347   1076.500000     53.1    987.000000   577.500000  10.400000  59.875000   \n",
      "9348    760.000000     93.0   1129.000000   905.000000   9.550000  63.150000   \n",
      "9349    470.250000    154.6   1600.000000  1457.250000   9.675000  61.924999   \n",
      "9350    414.750000    173.6   1776.500000  1704.500000  13.550000  48.875000   \n",
      "9351    506.750000    186.5   1375.250000  1582.500000  18.150001  36.275001   \n",
      "9352    538.500000    189.8   1374.250000  1728.500000  21.850000  29.250000   \n",
      "9353    603.750000    179.2   1263.500000  1269.000000  24.325000  23.725000   \n",
      "9354    603.250000    174.7   1240.750000  1092.000000  26.900000  18.350000   \n",
      "9355    701.500000    155.7   1041.000000   769.750000  28.325000  13.550000   \n",
      "9356    654.000000    167.7   1128.500000   816.000000  28.500000  13.125000   \n",
      "\n",
      "            AH  \n",
      "0     0.757754  \n",
      "1     0.725487  \n",
      "2     0.750239  \n",
      "3     0.786713  \n",
      "4     0.788794  \n",
      "5     0.784772  \n",
      "6     0.760312  \n",
      "7     0.770238  \n",
      "8     0.764819  \n",
      "9     0.751657  \n",
      "10    0.746495  \n",
      "11    0.736560  \n",
      "12    0.735295  \n",
      "13    0.741736  \n",
      "14    0.740795  \n",
      "15    0.769111  \n",
      "16    0.755183  \n",
      "17    0.735161  \n",
      "18    0.795054  \n",
      "19    0.839268  \n",
      "20    0.873589  \n",
      "21    0.877784  \n",
      "22    0.856938  \n",
      "23    0.818501  \n",
      "24    0.806544  \n",
      "25    0.831921  \n",
      "26    0.813314  \n",
      "27    0.741924  \n",
      "28    0.690484  \n",
      "29    0.665744  \n",
      "...        ...  \n",
      "9327  0.620458  \n",
      "9328  0.587528  \n",
      "9329  0.567283  \n",
      "9330  0.559414  \n",
      "9331  0.530155  \n",
      "9332  0.500166  \n",
      "9333  0.462436  \n",
      "9334  0.419179  \n",
      "9335  0.386566  \n",
      "9336  0.594491  \n",
      "9337  0.760784  \n",
      "9338  0.807281  \n",
      "9339  0.864238  \n",
      "9340  0.857933  \n",
      "9341  0.849727  \n",
      "9342  0.827454  \n",
      "9343  0.805778  \n",
      "9344  0.792730  \n",
      "9345  0.788769  \n",
      "9346  0.774275  \n",
      "9347  0.754964  \n",
      "9348  0.753129  \n",
      "9349  0.744608  \n",
      "9350  0.755337  \n",
      "9351  0.748652  \n",
      "9352  0.756824  \n",
      "9353  0.711864  \n",
      "9354  0.640649  \n",
      "9355  0.513866  \n",
      "9356  0.502804  \n",
      "\n",
      "[9357 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "air_quality.sample(5)\n",
    "print(air_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable corresponds to the CO(GT) variable of the first column. The following columns correspond to the variables in the feature vectors, *e.g.*, PT08.S1(CO) is $x_1$ up until AH which is $x_D$. The original dataset also has a date and a time columns that we are not going to use in this assignment.\n",
    "\n",
    "Before designing our predictive model, we need to think about three stages: the preprocessing stage, the training stage and the validation stage. The three stages are interconnected and *it is important to remember that the testing data that we use for validation has to be set aside before preprocessing*. Any preprocessing that you do has to be done only on the training data and several key statistics need to be saved for the test stage.\n",
    "\n",
    "Separating the dataset into training and test before any preprocessing has happened help us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing.\n",
    "\n",
    "We are going to use *hold-out validation* for testing our predictive model so we need to separate the dataset into a training set and a test set.\n",
    "\n",
    "### Question 3: Splitting the dataset (1 mark)\n",
    "\n",
    "Split the dataset into a training set and a test set. The training set should have 70% of the total observations and the test set, the 30%. For making the random selection make sure that you use a random seed that corresponds to the last five digits of your student UCard. Make sure that you comment your code.\n",
    "\n",
    "#### Question 3 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7485, 13)\n",
      "(1872, 13)\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#Setting a random seed for shuffling the data\n",
    "StudentID = 27624\n",
    "np.random.seed(StudentID)\n",
    "\n",
    "#Shuffling the data\n",
    "random_air_quality = air_quality.sample(frac=1,random_state = StudentID)\n",
    "\n",
    "\n",
    "#Spliting the data\n",
    "train_data = random_air_quality.iloc[:int(random_air_quality.shape[0]*0.8)]\n",
    "print(train_data.shape)\n",
    "test_data = random_air_quality.iloc[int(random_air_quality.shape[0]*0.8):]\n",
    "print(test_data.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "The dataset has missing values tagged with a -200 value. Before doing any work with the training data, we want to make sure that we deal properly with the missing values. \n",
    "\n",
    "### Question 4: Missing values (3 marks)\n",
    "\n",
    "Make some exploratory analysis on the number of missing values per column in the training data. \n",
    "\n",
    "* Remove the rows for which the target feature has missing values. We are doing supervised learning so we need all our data observations to have known target values.\n",
    "\n",
    "* Remove features with more than 20% of missing values. For all the other features with missing values, use the mean value of the non-missing values for imputation.\n",
    "\n",
    "#### Question 4 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Normalising the training data (2 marks)\n",
    "\n",
    "Now that you have removed the missing data, we need to normalise the input vectors. \n",
    "\n",
    "* Explain in a sentence why do you need to normalise the input features for this dataset.\n",
    "\n",
    "* Normalise the training data by substracting the mean value for each feature and dividing the result by the standard deviation of each feature. Keep the mean values and standard deviations, you will need them at test time.\n",
    "\n",
    "#### Question 5 Answer\n",
    "\n",
    "Write your explanation in this box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation stages\n",
    "\n",
    "We have now curated our training data by removing data observations and features with a large amount of missing values. We have also normalised the feature vectors. We are now in a good position to work on developing the prediction model and validating it. We will use both the closed form expression for $\\mathbf{w}$ and gradient descent for iterative optimisation. \n",
    "\n",
    "We first organise the dataframe into the vector of targets $\\mathbf{y}$ and the design matrix $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to get y and X\n",
    "y = \n",
    "X = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: training with closed form expression for $\\mathbf{w}$ (3 marks)\n",
    "\n",
    "To find the optimal value of $\\mathbf{w}$ using the closed form expression that you derived before, we need to know the value of the regularisation parameter $\\alpha$ in advance. We will determine the value by using part of the training data for finding the parameters $\\mathbf{w}$ and another part of the training data to choose the best $\\alpha$ from a set of predefined values.\n",
    "\n",
    "* Use `np.log(start, stop, num)` to create a set of values for $\\alpha$ in log scale. Use the following parameters `start=-3`, `stop=2` and `num=20`. \n",
    "\n",
    "* Randomly split the training data into what is properly called the training set and the validation set. As before, make sure that you use a random seed that corresponds to the last five digits of your student UCard. Use 70% of the data for the training set and 30% of the data for the validation set.\n",
    "\n",
    "* For each value that you have for $\\alpha$ from the previous step, use the training set to compute $\\mathbf{w}$ and then measure the mean-squared error (MSE) over the validation data. After this, you will have `num=20` MSE values. Choose the value of $\\alpha$ that leads to the lower MSE and save it. You will use it at the test stage.\n",
    "\n",
    "* What was the best value of $\\alpha$? Is there any explanation for that?\n",
    "\n",
    "#### Question 6 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to the last question here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: validation with the closed form expression for $\\mathbf{w}$ (2 marks)\n",
    "\n",
    "We are going to deal now with the test data to perform the validation of the model. Remember that the test data might also contain missing values in the target variable and in the input features.\n",
    "\n",
    "* Remove the rows of the test data for which the labels have missing values. \n",
    "* If you remove any feature at the training stage, you also need to remove the same features from the test stage.\n",
    "* Replace the missing values on each feature variables with the mean value you computed in the training data.\n",
    "* Normalise the test data using the means and standard deviations computed from the training data\n",
    "* Compute again $\\mathbf{w}$ for the value of $\\alpha$ that best performed on the validation set using ALL the training data (not all the training set).\n",
    "* Report the MSE on the preprocessed test data and an histogram with the absolute error.\n",
    "* Does the regularisation have any effect on the model? Explain your answer.\n",
    "\n",
    "#### Question 7 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the explanation to your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: training with gradient descent and validation (5 marks)\n",
    "\n",
    "\n",
    "Use gradient descent to iteratively compute the value of $\\mathbf{w}_{\\text{new}}$. Instead of using all the training set to compute the gradient, use a subset of $B$ datapoints in the training set. This is sometimes called minibatch gradient descent where $B$ is the size of the minibacth. When using gradient descent with minibatches, you need to find the best values for three parameters: $\\eta$, the learning rate, $B$, the number of datapoints in the minibatch and $\\alpha$, the regularisation parameter.\n",
    "\n",
    "* As you did on Question 6, create a grid of values for the parameters $\\alpha$ and $\\eta$ using `np.logspace` and a grid of values for $B$ using np.linspace. Because you need to find \n",
    " three parameters, start with `num=5` and see if you can increase it.\n",
    "\n",
    "* Use the same training set and validation set that you used in Question 6.\n",
    "\n",
    "* For each value that you have of $\\alpha$, $\\eta$ and $B$ from the previous step, use the training set to compute $\\mathbf{w}$ using minibatch gradient descent and then measure the MSE over the validation data. For the minibatch gradient descent choose to stop the iterative procedure after $500$ iterations.\n",
    "\n",
    "* Choose the values of $\\alpha$, $\\eta$ and $B$ that lead to the lower MSE and save them. You will use them at the test stage.\n",
    "\n",
    "*3 marks of out of the 5 marks*\n",
    "\n",
    "\n",
    "* Use the test set from Question 7 and provide the MSE obtained by having used minibatch training with the best values for $\\alpha$, $\\eta$ and $B$ over the WHOLE training data (not only the training set).\n",
    "\n",
    "* Compare the performance of the closed form solution and the minibatch solution. Are the performances similar? Are the parameters $\\mathbf{w}$ and $\\alpha$ similar in both approaches? Please comment on both questions.\n",
    "\n",
    "*2 marks of out of the 5 marks*\n",
    "\n",
    "#### Question 8 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code for your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the answer to your last question here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
