{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Quiz 2\n",
    "\n",
    "# You are strongly encourged to do the mock quiz first before reading the following solutions. \n",
    "\n",
    "## Scroll down for solutions if you are sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "**In Bayesian regression, what do we need to specify instead of to compute?**\n",
    "\n",
    "**Answer**: We need to specify the prior. Placing prior on model parameters is the definition of Bayesian inference approach.\n",
    "\n",
    "\n",
    "## Q2\n",
    "**An eigenvector of a covariance matrix tells you what?**\n",
    "\n",
    "**Answer**: An eigenvector defines the direction of variability and the corresponding eigenvalue is the corresponding amount/magnitude of variability (the variance captured).\n",
    "\n",
    "## Q3\n",
    "**Consider the following dataset: A = (0, 2), B = (0, 1) and C = (1, 0). The k-means algorithm is initialized with centers at A and B. Upon convergence, the two centers will be at**\n",
    "\n",
    "**Answer**: Get a piece of paper to draw the three points. Start with centers A and B. Iterate: 1) assign closest points; 2) update centers. You will find it converges very soon at A and the midpoint of BC.\n",
    "\n",
    "\n",
    "## Q4\n",
    "**We have a 2x1 feature vector. The first feature is X1 and the second feature is X2. The corresponding 2x2 covariance matrix is C = [9, 2; 2, 1], i.e., the first row is [9 2] and the second row is [2, 1]. What is the standard deviation of X1?**\n",
    "\n",
    "**Answer**: The variance of X1 is C(1,1)=9 so the std is $\\sqrt{9}=3$.\n",
    "\n",
    "## Q5\n",
    "**We model a 3x1 feature vector using independent multivariate Gaussian distribution. How many parameters do we need to specify this multivariate distribution?**\n",
    "\n",
    "**Answer**: The mean of this vector needs three parameters, one for each feature. Due to the independence, the covariance matrix is diagonal, with the variance of each feature on the diagonal and three variance parameters in total. Therefore, the total is 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "**Please modify Lab 6B striclty following the following requirements from a client:...**\n",
    "\n",
    "**Answer**: Please see below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Linear Regression using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start right away with implementing linear regression in PyTorch to study PyTorch concepts closely. This part follows the [PyTorch Linear regression example](https://github.com/pytorch/examples/tree/master/regression) that trains a **single fully-connected layer** to fit a 4th degree polynomial.\n",
    "\n",
    "### A synthetic linear regression problem\n",
    "\n",
    "* Generate model parameters, weight and bias. The weight vector and bias are both tensors, 1D and 0D, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5937],\n",
      "        [ 1.0550],\n",
      "        [ 3.7313],\n",
      "        [-3.0678],\n",
      "        [-0.5928],\n",
      "        [ 7.7825]])\n",
      "tensor([6.8310])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "POLY_DEGREE = 6\n",
    "#Set the random seed for reproducibility \n",
    "torch.manual_seed(2019) \n",
    "W_target = torch.randn(POLY_DEGREE, 1) * 5\n",
    "b_target = torch.randn(1) * 5\n",
    "print(W_target)\n",
    "print(b_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, define a number of functions to generate the input (variables) and output (target/response). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(x):\n",
    "    \"\"\"Builds features i.e. a matrix with columns [x, x^2, x^3, x^4].\"\"\"\n",
    "    x = x.unsqueeze(1)\n",
    "    return torch.cat([x ** i for i in range(1, POLY_DEGREE+1)], 1)\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Approximated function.\"\"\"\n",
    "    return x.mm(W_target) + b_target.item()\n",
    "\n",
    "def poly_desc(W, b):\n",
    "    \"\"\"Creates a string description of a polynomial.\"\"\"\n",
    "    result = 'y = '\n",
    "    for i, w in enumerate(W):\n",
    "        result += '{:+.2f} x^{} '.format(w, len(W) - i)\n",
    "    result += '{:+.2f}'.format(b[0])\n",
    "    return result\n",
    "\n",
    "def get_batch(batch_size=32):\n",
    "    \"\"\"Builds a batch i.e. (x, f(x)) pair.\"\"\"\n",
    "    random = torch.randn(batch_size)\n",
    "    x = make_features(random)\n",
    "    y = f(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a simple(st) neural network, which is a **single fully connected** (FC) layer. See [`torch.nn.Linear`](https://pytorch.org/docs/master/nn.html#torch.nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=6, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "fc = torch.nn.Linear(W_target.size(0), 1)\n",
    "print(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This is a *network* with four input units, one output unit, with a bias term.\n",
    "    \n",
    "* Now generate the data. Let us try to get five pairs of (x,y) first to inspect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0199e+00,  1.0401e+00,  1.0608e+00,  1.0819e+00,  1.1034e+00,\n",
      "          1.1253e+00],\n",
      "        [-1.0375e+00,  1.0763e+00, -1.1166e+00,  1.1585e+00, -1.2019e+00,\n",
      "          1.2469e+00],\n",
      "        [-7.2291e-01,  5.2260e-01, -3.7779e-01,  2.7311e-01, -1.9743e-01,\n",
      "          1.4273e-01],\n",
      "        [-1.0365e+00,  1.0743e+00, -1.1135e+00,  1.1541e+00, -1.1962e+00,\n",
      "          1.2399e+00],\n",
      "        [-1.8514e-01,  3.4276e-02, -6.3457e-03,  1.1748e-03, -2.1750e-04,\n",
      "          4.0268e-05],\n",
      "        [-1.5111e+00,  2.2835e+00, -3.4506e+00,  5.2143e+00, -7.8794e+00,\n",
      "          1.1907e+01],\n",
      "        [ 1.6494e-01,  2.7204e-02,  4.4869e-03,  7.4005e-04,  1.2206e-04,\n",
      "          2.0132e-05],\n",
      "        [ 5.5548e-02,  3.0856e-03,  1.7140e-04,  9.5209e-06,  5.2887e-07,\n",
      "          2.9378e-08],\n",
      "        [-1.2363e+00,  1.5285e+00, -1.8898e+00,  2.3364e+00, -2.8885e+00,\n",
      "          3.5712e+00],\n",
      "        [ 6.2126e-01,  3.8596e-01,  2.3978e-01,  1.4896e-01,  9.2544e-02,\n",
      "          5.7494e-02]])\n",
      "tensor([[16.0656],\n",
      "        [11.2784],\n",
      "        [ 6.7919],\n",
      "        [11.2429],\n",
      "        [ 6.9503],\n",
      "        [78.6006],\n",
      "        [ 6.7764],\n",
      "        [ 6.8019],\n",
      "        [24.4640],\n",
      "        [ 7.6997]])\n"
     ]
    }
   ],
   "source": [
    "sample_x, sample_y = get_batch(10)\n",
    "print(sample_x)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Reset the gradients to zero, perform a forward pass to get prediction, and compute the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.689815521240234\n"
     ]
    }
   ],
   "source": [
    "fc.zero_grad()\n",
    "output = F.smooth_l1_loss(fc(sample_x), sample_y)\n",
    "loss1 = output.item()\n",
    "print(loss1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Random did not give a good prediction. Let us do a backpropagation and update model parameters with gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward() \n",
    "for param in fc.parameters():  \n",
    "    param.data.add_(-0.1 * param.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Check the updated weights and respective loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0705, -0.1716,  0.2372,  0.0800,  0.1448, -0.1230]],\n",
      "       requires_grad=True)\n",
      "17.817602157592773\n"
     ]
    }
   ],
   "source": [
    "print(fc.weight)\n",
    "output = F.smooth_l1_loss(fc(sample_x), sample_y)\n",
    "loss2 = output.item()\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8722133636474609\n"
     ]
    }
   ],
   "source": [
    "print(loss1-loss2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
